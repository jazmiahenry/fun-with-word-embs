{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e69422fc",
   "metadata": {},
   "source": [
    "# Large Language Model Training Tools\n",
    "\n",
    "Textual  data can be quite difficult to clean. This notebook is a tool to assist students with best practices to cleaning textual data before performing analysis or doing model training. As we see in the tokenization with word embeddings notebook, analyzing your data before properly cleaning it can give you worse results and stop your model from performing as well as it could. Not only that, but you can introduce hateful words and phrases into your trained model if you do not take the care to clean your data appropriately. \n",
    "\n",
    "This notebook will show you how to:\n",
    "\n",
    "1. Clean data that has been ingested in a .txt file\n",
    "2. Remove punctuation, numbers and stopwords\n",
    "3. Create Counterfactual Datasets\n",
    "4. Remove (prune) hateful words and phrases that have been tagged\n",
    "5. Flag words and phrases for tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7f9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Iterable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a902749",
   "metadata": {},
   "source": [
    "## Saving Data From .txt File\n",
    "\n",
    "Many text data sets are stored in .txt files due to their relatively light weight compared to a .csv file. To save data coming from a .txt file, follow the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f412c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [] # list that you will add each line from the .txt line to\n",
    "\n",
    "# Open connection to file\n",
    "with open(\"<file>.txt\", \"r\") as file: # add file \n",
    "    # Read in tweets and store in list\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        ls.append(data) #add the lines to your list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5df8153a",
   "metadata": {},
   "source": [
    "## Cleaning Data Inside of More than One File\n",
    "\n",
    "In the *word-embs-fun.ipynb* file, I show how to clean a single .txt file and save as a dictionary. Below, I show you how to save multiple files into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b713659",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'src/<path>' # designate the path where your data is houses\n",
    "\n",
    "format_txt = os.path.join(path, '*.txt')\n",
    "\n",
    "file_list = glob.glob(format_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34dcab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(ls):\n",
    "    file_list = []\n",
    "    for files in file_list:\n",
    "        with open(files) as f:\n",
    "            data = f.read()\n",
    "            data = data.lower()          \n",
    "            data = re.sub(r'\\d+', '', data) #removes numbers\n",
    "            newdata = re.sub(r'[^\\w\\s]','',data) #removes punctuation\n",
    "            data = re.sub('â,â,â', ' ', newdata, re.IGNORECASE) #removes special characters\n",
    "            filename = 'corpora.txt' \n",
    "        with open(filename, 'w') as f:\n",
    "            f.writelines(data)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2265bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "PreProcess(file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9035966d",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "\n",
    "Stopwords are commonly used words a language that do not actively contribute to the meaning of a sentence. Top stopwords are articles such as 'and', the' and 'at'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7394d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(data)\n",
    "result = [i for i in tokens if not i in stop_words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "823ac5f6",
   "metadata": {},
   "source": [
    "## Create Counterfactual Dataset\n",
    "In Stage Two of Our Chapter, we discuss counterfactuals. You can create datasets that replace target words with non-target words to test if a model will provide the same output it would have if the target words would have been left in tact. \n",
    "\n",
    "Once you have created the counterfactual dataset, run your model on both the counterfactual dataset and the original dataset to see if your model scores each sentiment the same on specific tasks. \n",
    "\n",
    "Tools such as DiCE (https://github.com/interpretml/DiCE) and TensorFlow Responsible (https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/creating_a_custom_counterfactual_dataset) have more information on how to train a counterfactual model using counterfactual data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82043677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counterfactual Dataset\n",
    "\n",
    "replacement = {\"hate\": \"love\", \"bad\": \"good\", \"ugly\": \"beauty\"}\n",
    "\n",
    "\n",
    "df[\"column\"].replace(replacement, inplace=True)\n",
    "\n",
    "# if you want to save replacement data as a new column instead of inplace\n",
    "\n",
    "df[\"counter_column\"] = df[\"column\"].replace(replacement, inplace=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "120ecd2b",
   "metadata": {},
   "source": [
    "## Remove (Prune) Hateful Words In a String Within a List\n",
    "\n",
    "If you have a list of hateful words, you can prune them from your dataset using this code. We learn about the benefits of pruning in Stage Three of the Chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3121677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = ['hate', 'bad', 'ugly']\n",
    "\n",
    "words = [w.sub(ls, ' ') for w in words] #removes all words from list and replaces with a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also replace hateful words with better ones\n",
    "ls = ['hate', 'bad', 'ugly']\n",
    "\n",
    "rep = ['love', 'good', 'beautiful']\n",
    "\n",
    "words = [w.replace(ls, rep) for w in words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d36ed4f",
   "metadata": {},
   "source": [
    "## Flag and Tag Words \n",
    "\n",
    "You can flag hateful words in a dataset and even create a new column designating whether a flagged word is in a given sentence. We discuss the benefits of flagging words in a dataset in Stage Three of the Chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag hateful words in a \"flagged\" column\n",
    "\n",
    "ls = ['hate', 'bad', 'ugly']\n",
    "\n",
    "\n",
    "hate_list = '|'.join([x.strip() for x in words])\n",
    "\n",
    "#remove words in the list and \n",
    "def flagged_column(df, column, hate_list):\n",
    "    #convert True to 1 and False to 0\n",
    "    df[\"column\"] = df[column].str.contains(hate_list, case=False, na=False).astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_column(df_name, \"flagged_column_name\", ls) # add df name, name of new flagged column and list of words to flag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "075d6747",
   "metadata": {},
   "source": [
    "## Let's Think\n",
    "\n",
    "What are some ways that these functions can be used to improve your projects using Large Language Models? \n",
    "\n",
    "What are you excited to create now that you have some tools to begin exploring Large Language Model training responsibly?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f57ffd79f07881f90ffd84d1ee449596c2bc3e88fee236dc006178dc960802e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
